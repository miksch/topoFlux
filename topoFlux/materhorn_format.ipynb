{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('base': conda)",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2b77333d2baaac57be0f1fc52442a62a779f11f3138eda198b86e9cbc64a3206"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from functools import partial\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, progress\n",
    "#client = Client(n_workers=2, threads_per_worker=2, memory_limit='4GB')\n",
    "#client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = \"C:/Users/moo90/Box/data/materhorn/raw_data/ES\"\n",
    "\n",
    "parq_path = \"C:/Users/moo90/Box/data/materhorn/ES/parq\"\n",
    "csv_path = \"C:/Users/moo90/Box/data/materhorn/ES/csv\"\n",
    "\n",
    "# UofU : var_hgt (w/ decimals)\n",
    "# Split on underscore, index by first and last\n",
    "# first -> use rename dict\n",
    "# last -> convert to float, then centimeters, then centimeters to int\n",
    "\n",
    "# UND : varhgt_?? (w/o decimals)\n",
    "# Split on underscore, use first, then split by numerical vs. alphabetic\n",
    "# alpha -> use rename dict\n",
    "# num -> convert to centimeters, then centimeters to int\n",
    "\n",
    "\n",
    "def find_time_lims(fpath, dt_str='%Y%m%d%H%M%S'):\n",
    "    \"\"\"\n",
    "    Find start and end dates for each file based on naming conventions for materhorn dataset\n",
    "\n",
    "    Args:\n",
    "        fpath (str) : full file name or path to dataset\n",
    "        dt_str (str) : strptime representation of the date in the filename\n",
    "\n",
    "    Returns:\n",
    "        list containing start [0] and end [1] of timeseries in file\n",
    "    \"\"\"\n",
    "\n",
    "    dt_list = os.path.splitext(os.path.basename(fpath))[0].split('_')[-2:]\n",
    "\n",
    "    return [pd.to_datetime(dt, format=dt_str) for dt in dt_list]\n",
    "\n",
    "def asdf(file_list, rename_func, in_path, parq_path, csv_path, station_pairs=None):\n",
    "\n",
    "    file_dict = {k:{'start_end_dates':find_time_lims(k)} for k in file_list}\n",
    "\n",
    "    date_list = []\n",
    "\n",
    "    # Make sure output directories are there\n",
    "    os.makedirs(os.path.join(parq_path, \"raw\", station), exist_ok=True)\n",
    "    os.makedirs(os.path.join(csv_path, \"raw\", station), exist_ok=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ES1 - UofU (easiest)\n",
    "\n",
    "def rename_es1(col_str):\n",
    "    \n",
    "    var_rename = {'Ux' : 'u',\n",
    "                  'Uy' : 'v',\n",
    "                  'Uz' : 'w',\n",
    "                  'T_Sonic' : 't_sonic',\n",
    "                  'diagnostic' : 'diag_sonic'\n",
    "                  }\n",
    "\n",
    "    (vname, _, hgt) = col_str.rpartition('_')\n",
    "    new_vname = var_rename[vname]\n",
    "    new_hgt = int(float(hgt) * 100)\n",
    "\n",
    "    return f\"{new_vname}_{new_hgt}\"\n",
    "\n",
    "station = 'ES1'\n",
    "\n",
    "# First find files and start/end dates of each file\n",
    "file_list = sorted(glob.glob(os.path.join(in_path, station, f\"DPG-UoU_{station}_*_20Hz_*.txt\")))\n",
    "\n",
    "file_dict = {k:{'start_end_dates':find_time_lims(k)} for k in file_list}\n",
    "\n",
    "date_list = []\n",
    "\n",
    "# Make sure output directories are there\n",
    "os.makedirs(os.path.join(parq_path, \"raw\", station), exist_ok=True)\n",
    "os.makedirs(os.path.join(csv_path, \"raw\", station), exist_ok=True)\n",
    "\n",
    "for i,f in enumerate(file_dict.keys()):\n",
    "\n",
    "    # Read in file\n",
    "    in_df = pd.read_csv(f, skiprows=[0,2,3], header=0, index_col=0, parse_dates=[0], na_values=[\"NAN\"])\n",
    "    \n",
    "    # Drop duplicates, force to 20hz, and shift one\n",
    "    in_df = in_df[~in_df.index.duplicated(keep='first')].shift(-1, freq='50ms')\n",
    "\n",
    "    # Write initial running dataframe or concat to existing\n",
    "    if i==0:\n",
    "        running_df = in_df.copy()\n",
    "    else:\n",
    "        running_df = pd.concat([in_df, running_df])\n",
    "\n",
    "    # Drop any dates already written out\n",
    "    # https://stackoverflow.com/questions/37307796/fastest-way-to-eliminate-specific-dates-from-pandas-dataframe\n",
    "    running_df = running_df.loc[~np.in1d(running_df.index.date, pd.to_datetime(date_list).date), :]\n",
    "\n",
    "    # Concat to running dataframe\n",
    "    out_dates = np.unique(running_df.index.date)\n",
    "\n",
    "    for j,date in enumerate(out_dates):\n",
    "        # Skip last date if not on last data file\n",
    "        if (j == out_dates.shape[0]-1) & (i != len(file_dict.keys())):\n",
    "            continue\n",
    "        \n",
    "        # Select date\n",
    "        temp_df = running_df.loc[date.strftime('%Y-%m-%d')]\n",
    "\n",
    "        # Shift back to correct datetimes, sort index\n",
    "        # Also force to 20hz between first and last value\n",
    "        temp_df = temp_df.shift(1, freq='50ms').sort_index().asfreq('50ms')\n",
    "\n",
    "        # Rename columns\n",
    "        temp_df = temp_df.rename(columns=rename_es1)\n",
    "    \n",
    "        # Write out to parquet file\n",
    "        file_date = date.strftime('%Y%m%d')\n",
    "        temp_df.to_parquet(os.path.join(parq_path, \"raw\", station, f\"{station}_EC_20hz_{file_date}_raw.parquet\"), engine='pyarrow', index=True)\n",
    "        temp_df.to_csv(os.path.join(csv_path, \"raw\", station, f\"{station}_EC_20hz_{file_date}_raw.csv\"), float_format='%g')\n",
    "\n",
    "        date_list.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ES2 - UND\n",
    "\n",
    "def rename_es2(col_str, file_id):\n",
    "    \n",
    "    var_rename = {'u' : 'u',\n",
    "                  'v' : 'v',\n",
    "                  'w' : 'w',\n",
    "                  'ts' : 't_sonic'\n",
    "                  }\n",
    "\n",
    "    try:\n",
    "        [vname, hgt] = re.findall(r\"[^\\W\\d_]+|\\d+\", col_str.rpartition('_')[0])\n",
    "        new_vname = var_rename[vname]\n",
    "\n",
    "        if hgt.startswith(\"0\"):\n",
    "            new_hgt = int(float(hgt) * 10)\n",
    "        else:\n",
    "            new_hgt = int(float(hgt) * 100)\n",
    "\n",
    "        return f\"{new_vname}_{new_hgt}\"\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        return f\"{col_str.lower()}_{file_id}\"\n",
    "\n",
    "# 1407 - Fall 0.5, 4, and 10 m\n",
    "# 2590 - Spring 16, 20, 25, and 32 m\n",
    "# 2717 - Fall 16, 20, 25, and 28 m and Spring 0.5, 2, 5, and 10 m\n",
    "# 2720 - Fall and Spring temperature profiles\n",
    "\n",
    "# Fall should zip fine\n",
    "# Spring is off by one (jfc)... decided to load all into memory\n",
    "station_pairs = {#2012 : [1407, 2717],\n",
    "                 2013 : [2717, 2590]\n",
    "                 }\n",
    "\n",
    "station = 'ES2'\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(os.path.join(parq_path, \"raw\", station), exist_ok=True)\n",
    "os.makedirs(os.path.join(csv_path, \"raw\", station), exist_ok=True)\n",
    "\n",
    "for yr in station_pairs.keys():\n",
    "\n",
    "    yr_list = []\n",
    "\n",
    "    for fid in station_pairs[yr]:\n",
    "\n",
    "        temp_df_list = []\n",
    "\n",
    "        # Get file list for certain file id and year\n",
    "        file_list = glob.glob(os.path.join(in_path, station, 'raw_20hz', f\"UND_{station}_{fid}*_{yr}*.txt\"))\n",
    "\n",
    "        for f in file_list:\n",
    "            \n",
    "            # Read in data from file, remove any duplicates, shift -1 for date grouping, and rename columns\n",
    "            temp_df = pd.read_csv(f, skiprows=[0,2,3], header=0, na_values=[\"NAN\"], parse_dates=[0], index_col=[0])\n",
    "            temp_df = temp_df[~temp_df.index.duplicated(keep='first')].shift(-1, freq='50ms').rename(columns=partial(rename_es2, file_id=fid))\n",
    "\n",
    "            # Append to running list\n",
    "            temp_df_list.append(temp_df)\n",
    "\n",
    "        # Concat all dataframes with same year and \n",
    "        yr_list.append(pd.concat(temp_df_list))\n",
    "\n",
    "    # Concat all dataframes for the year, ort and pad any missing values with nans\n",
    "    yr_df = pd.concat(yr_list, axis=1)\n",
    "    yr_list = []\n",
    "    yr_df = yr_df.sort_index().asfreq('50ms')\n",
    "\n",
    "    # Get dates to loop through (shifting once just in case... although it's a very isolated edge case)\n",
    "    out_dates = np.unique(yr_df.shift(1, freq='50ms').index.date)\n",
    "\n",
    "    for date in out_dates:\n",
    "\n",
    "        # Subset by days, shift back times, and rename columns\n",
    "        daily_df = yr_df.loc[date.strftime('%Y-%m-%d')].shift(1, freq='50ms')\n",
    "\n",
    "        # Write out to parquet and csv files\n",
    "        file_date = date.strftime('%Y%m%d')\n",
    "        daily_df.to_parquet(os.path.join(parq_path, \"raw\", station, f\"{station}_EC_20hz_{file_date}_raw.parquet\"), engine='pyarrow', index=True)\n",
    "        daily_df.to_csv(os.path.join(csv_path, \"raw\", station, f\"{station}_EC_20hz_{file_date}_raw.csv\"), float_format='%g')\n",
    "\n",
    "    yr_df = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Shape of passed values is (66897838, 28), indices imply (65747694, 28)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b2977450db1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;31m# Concat all dataframes for the year, ort and pad any missing values with nans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0myr_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myr_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m     \u001b[0myr_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0myr_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myr_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masfreq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'50ms'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    285\u001b[0m     )\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    500\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             new_data = concatenate_block_managers(\n\u001b[0m\u001b[0;32m    503\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;31m# Populate known_consolidate, blknos, and blklocs lazily\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (66897838, 28), indices imply (65747694, 28)"
     ]
    }
   ],
   "source": [
    "# ES3 - UND\n",
    "\n",
    "# 2590 - Fall temperature profiles\n",
    "# 5254 - Spring temperature profiles\n",
    "# 2667 - Fall and Spring 5, 10, and 20 m \n",
    "# 2712 - Fall and Spring 0.5 and 2 m (NOTE: 2m has different naming conventions (yay...))\n",
    "    # Fall \"TIMESTAMP\",\"RECORD\",\"u05_EM\",\"v05_EM\",\"w05_EM\",\"ts05_EM\",\"T2_EM\",\"Ux2_EM\",\"Uy2_EM\",\"Uz2_EM\",\"Ts2_EM\",\"rho_w\",\"kh_mV\",\"PTemp\",\"VWC\",\"Period\"\n",
    "    # Spring \"TIMESTAMP\",\"RECORD\",\"u05_ES3\",\"v05_ES3\",\"w05_ES3\",\"Ts05_ES3\",\"T2_ES3\",\"ux2_ES3\",\"Uy2_ES3\",\"Uz2_ES3\",\"Ts2_ES3\",\"CO2_ES3\",\"H2O_ES3\",\"CO2abs_ES3\",\"H2Oabs_ES3\",\"P7500_ES3\",\"T7500_ES3\",\"Cool7500_ES3\",\"Diag7500_ES3\",\"PTemp\"\n",
    "\n",
    "def rename_es3(col_str, file_id):\n",
    "    \n",
    "    var_rename_hgt = {'u' : 'u',\n",
    "                      'v' : 'v',\n",
    "                      'w' : 'w',\n",
    "                      'ux' : 'u',\n",
    "                      'uy' : 'v',\n",
    "                      'uz' : 'w',\n",
    "                      'ts' : 't_sonic'\n",
    "                     }\n",
    "\n",
    "    var_rename_nohgt = {'rho_w' : 'kh2o_h2o_200',\n",
    "                        'kh_mv' : 'kh2o_mv_200',\n",
    "                        'CO2_ES3' : 'li_co2_200',\n",
    "                        'H2O_ES3' : 'li_h2o_200',\n",
    "                        'CO2abs_ES3' : 'li_co2_abs_200',\n",
    "                        'H2Oabs_ES3' : 'li_h2o_abs_200',\n",
    "                        'P7500_ES3' : 'li_pres_200',\n",
    "                        'T7500_ES3' : 'li_t_200',\n",
    "                        'Cool7500_ES3' : 'li_cool_200',\n",
    "                        'Diag7500_ES3' : 'li_diag_200'\n",
    "                       }\n",
    "\n",
    "    # for sonic variables\n",
    "    try:\n",
    "        [vname, hgt] = re.findall(r\"[^\\W\\d_]+|\\d+\", col_str.rpartition('_')[0].lower())\n",
    "        new_vname = var_rename_hgt[vname]\n",
    "\n",
    "        if hgt.startswith(\"0\"):\n",
    "            new_hgt = int(float(hgt) * 10)\n",
    "        else:\n",
    "            new_hgt = int(float(hgt) * 100)\n",
    "\n",
    "        return f\"{new_vname}_{new_hgt}\"\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        # for kh2o and li7500 variables\n",
    "        try:\n",
    "            return var_rename_nohgt[col_str]\n",
    "        \n",
    "        # any others\n",
    "        except:\n",
    "            return f\"{col_str.lower()}_{file_id}\"\n",
    "\n",
    "\n",
    "# Force lowercase for Ts/ts comparison\n",
    "\n",
    "\n",
    "# UND_ES3_2590_20Hz_FluxTower_20120924184437_20120925162426.txt\n",
    "\n",
    "station_pairs = {2012: [2712, 2667],\n",
    "                 2013: [2712, 2667]\n",
    "                 }\n",
    "\n",
    "station = 'ES3'\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(os.path.join(parq_path, \"raw\", station), exist_ok=True)\n",
    "os.makedirs(os.path.join(csv_path, \"raw\", station), exist_ok=True)\n",
    "\n",
    "# Follow same processing steps as ES2 (because again, the file lists are not the same length)\n",
    "for yr in station_pairs.keys():\n",
    "\n",
    "    yr_list = []\n",
    "\n",
    "    for fid in station_pairs[yr]:\n",
    "\n",
    "        temp_df_list = []\n",
    "\n",
    "        # Get file list for certain file id and year\n",
    "        file_list = glob.glob(os.path.join(in_path, station, 'raw_20hz', f\"UND_{station}_{fid}*_{yr}*.txt\"))\n",
    "\n",
    "        for f in file_list:\n",
    "            \n",
    "            # Read in data from file, remove any duplicates, shift -1 for date grouping, and rename columns\n",
    "            temp_df = pd.read_csv(f, skiprows=[0,2,3], header=0, na_values=[\"NAN\"], parse_dates=[0], index_col=[0])\n",
    "            temp_df = temp_df[~temp_df.index.duplicated(keep='first')].shift(-1, freq='50ms').rename(columns=partial(rename_es3, file_id=fid))\n",
    "\n",
    "            # Append to running list\n",
    "            temp_df_list.append(temp_df)\n",
    "\n",
    "        # Concat all dataframes with same year and \n",
    "        yr_list.append(pd.concat(temp_df_list))\n",
    "\n",
    "    # Concat all dataframes for the year, ort and pad any missing values with nans\n",
    "    yr_df = pd.concat(yr_list, axis=1)\n",
    "    yr_list = []\n",
    "    yr_df = yr_df.sort_index().asfreq('50ms')\n",
    "\n",
    "    # Get dates to loop through (shifting once just in case... although it's a very isolated edge case)\n",
    "    out_dates = np.unique(yr_df.shift(1, freq='50ms').index.date)\n",
    "\n",
    "    for date in out_dates:\n",
    "\n",
    "        # Subset by days, shift back times, and rename columns\n",
    "        daily_df = yr_df.loc[date.strftime('%Y-%m-%d')].shift(1, freq='50ms')\n",
    "\n",
    "        # Write out to parquet and csv files\n",
    "        file_date = date.strftime('%Y%m%d')\n",
    "        daily_df.to_parquet(os.path.join(parq_path, \"raw\", station, f\"{station}_EC_20hz_{file_date}_raw.parquet\"), engine='pyarrow', index=True)\n",
    "        daily_df.to_csv(os.path.join(csv_path, \"raw\", station, f\"{station}_EC_20hz_{file_date}_raw.csv\"), float_format='%g')\n",
    "\n",
    "    yr_df = None\n",
    "\n",
    "# Clear out variables\n",
    "yr_df = None\n",
    "yr_list = None\n",
    "temp_df_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[                         record_2712   u_50   v_50   w_50  t_sonic_50  \\\nTIMESTAMP                                                               \n2012-09-22 06:37:19.150            0  0.207  1.797 -0.750       302.0   \n2012-09-22 06:37:19.200            1 -0.185  1.615 -0.640       301.9   \n2012-09-22 06:37:19.250            2  0.919  1.115 -0.506       302.0   \n2012-09-22 06:37:19.300            3  0.891  1.064 -0.077       302.1   \n2012-09-22 06:37:19.350            4  0.271  0.895 -0.262       302.0   \n...                              ...    ...    ...    ...         ...   \n2012-10-29 18:57:26.100     41201307  0.407 -1.022 -0.086       291.1   \n2012-10-29 18:57:26.150     41201308  0.345 -0.972 -0.025       291.3   \n2012-10-29 18:57:26.200     41201309  0.346 -0.972 -0.026       291.3   \n2012-10-29 18:57:26.250     41201310  0.295 -0.901 -0.123       291.3   \n2012-10-29 18:57:26.300     41201311  0.345 -0.839 -0.319       291.1   \n\n                         t2_em_2712  u_200  v_200  w_200  t_sonic_200  \\\nTIMESTAMP                                                               \n2012-09-22 06:37:19.150       27.75    NaN    NaN    NaN          NaN   \n2012-09-22 06:37:19.200       27.79    NaN    NaN    NaN          NaN   \n2012-09-22 06:37:19.250       27.84    NaN    NaN    NaN          NaN   \n2012-09-22 06:37:19.300       27.96    NaN    NaN    NaN          NaN   \n2012-09-22 06:37:19.350       28.01    NaN    NaN    NaN          NaN   \n...                             ...    ...    ...    ...          ...   \n2012-10-29 18:57:26.100       18.59 -0.213  0.820 -0.454        17.15   \n2012-10-29 18:57:26.150       18.60 -0.251  0.746 -0.421        17.13   \n2012-10-29 18:57:26.200       18.60 -0.221  0.709 -0.365        17.10   \n2012-10-29 18:57:26.250       18.64 -0.323  0.720 -0.407        17.16   \n2012-10-29 18:57:26.300       18.58 -0.255  0.928 -0.328        17.11   \n\n                         kh2o_h2o_200  kh_mv_2712  ptemp_2712  vwc_2712  \\\nTIMESTAMP                                                                 \n2012-09-22 06:37:19.150         0.643      3217.0       32.98     0.043   \n2012-09-22 06:37:19.200         0.720      3170.0       32.98     0.043   \n2012-09-22 06:37:19.250         0.647      3214.0       32.98     0.043   \n2012-09-22 06:37:19.300         0.744      3155.0       32.98     0.043   \n2012-09-22 06:37:19.350         0.742      3156.0       32.98     0.043   \n...                               ...         ...         ...       ...   \n2012-10-29 18:57:26.100           NaN      -274.7       19.54       NaN   \n2012-10-29 18:57:26.150           NaN      -274.6       19.53       NaN   \n2012-10-29 18:57:26.200           NaN      -274.6       19.53       NaN   \n2012-10-29 18:57:26.250           NaN      -274.6       19.54       NaN   \n2012-10-29 18:57:26.300           NaN      -274.7       19.53       NaN   \n\n                         period_2712  \nTIMESTAMP                             \n2012-09-22 06:37:19.150        17.76  \n2012-09-22 06:37:19.200        17.76  \n2012-09-22 06:37:19.250        17.75  \n2012-09-22 06:37:19.300        17.75  \n2012-09-22 06:37:19.350        17.76  \n...                              ...  \n2012-10-29 18:57:26.100          NaN  \n2012-10-29 18:57:26.150          NaN  \n2012-10-29 18:57:26.200          NaN  \n2012-10-29 18:57:26.250          NaN  \n2012-10-29 18:57:26.300          NaN  \n\n[65338088 rows x 15 columns],                          record_2667   u_500   v_500   w_500  t_sonic_500  \\\nTIMESTAMP                                                                   \n2012-09-21 22:57:54.500            0 -24.750 -21.430 -22.470        223.2   \n2012-09-21 22:57:54.550            1 -22.620 -21.590 -22.500        223.2   \n2012-09-21 22:57:54.600            2 -21.290 -21.670 -22.530        223.2   \n2012-09-21 22:57:54.650            3 -20.380 -21.690 -22.560        223.2   \n2012-09-21 22:57:54.700            4 -19.810 -21.680 -22.580        223.2   \n...                              ...     ...     ...     ...          ...   \n2012-10-29 18:57:35.350     41384568   0.860  -0.826  -0.035        290.1   \n2012-10-29 18:57:35.400     41384569   0.800  -0.935  -0.083        290.2   \n2012-10-29 18:57:35.450     41384570   0.859  -0.899  -0.070        290.2   \n2012-10-29 18:57:35.500     41384571   0.945  -0.921  -0.145        290.2   \n2012-10-29 18:57:35.550     41384572   0.897  -0.898  -0.181        290.2   \n\n                         u_1000  v_1000  w_1000  t_sonic_1000  u_2000  v_2000  \\\nTIMESTAMP                                                                       \n2012-09-21 22:57:54.500 -23.370 -23.640 -24.110         221.3 -21.620 -25.070   \n2012-09-21 22:57:54.550 -23.510 -23.660 -24.110         221.3 -21.630 -25.080   \n2012-09-21 22:57:54.600 -23.540 -23.680 -24.110         221.3 -21.640 -25.090   \n2012-09-21 22:57:54.650 -23.510 -23.700 -24.110         221.3 -21.650 -25.090   \n2012-09-21 22:57:54.700 -23.440 -23.720 -24.110         221.3 -21.650 -25.100   \n...                         ...     ...     ...           ...     ...     ...   \n2012-10-29 18:57:35.350   0.746  -1.048  -0.261         290.1   1.494  -0.939   \n2012-10-29 18:57:35.400   0.735  -1.096  -0.188         290.1   1.613  -0.828   \n2012-10-29 18:57:35.450   0.806  -1.146  -0.200         290.1   1.626  -0.793   \n2012-10-29 18:57:35.500   0.784  -1.096  -0.237         290.1   1.493  -0.768   \n2012-10-29 18:57:35.550   0.721  -1.156  -0.151         290.1   1.492  -0.768   \n\n                         w_2000  t_sonic_2000  \nTIMESTAMP                                      \n2012-09-21 22:57:54.500 -24.980         220.2  \n2012-09-21 22:57:54.550 -24.980         220.1  \n2012-09-21 22:57:54.600 -24.980         220.1  \n2012-09-21 22:57:54.650 -24.980         220.1  \n2012-09-21 22:57:54.700 -24.980         220.1  \n...                         ...           ...  \n2012-10-29 18:57:35.350   0.409         290.9  \n2012-10-29 18:57:35.400   0.385         290.9  \n2012-10-29 18:57:35.450   0.275         290.9  \n2012-10-29 18:57:35.500   0.446         290.9  \n2012-10-29 18:57:35.550   0.189         290.9  \n\n[60738567 rows x 13 columns]]\n"
     ]
    }
   ],
   "source": [
    "print(yr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ES4 - UofU\n",
    "\n",
    "# NOTE: tower fell down at one point\n",
    "# 3063 - Fall 0.5, 2, 5, 10, 20, and 32 m\n",
    "# 4004 - Spring 0.5, 2, 5, 10, 20, and 32 m (pt 1)\n",
    "# 5252 - Spring 0.5, 2, 5, 10, 20, and 32 m (pt 2)\n",
    "\n",
    "def rename_es4(col_str):\n",
    "    \n",
    "    var_rename = {'Ux' : 'u',\n",
    "                  'Uy' : 'v',\n",
    "                  'Uz' : 'w',\n",
    "                  'T_Sonic' : 't_sonic',\n",
    "                  'diagnostic' : 'diag_sonic'\n",
    "                  }\n",
    "\n",
    "    (vname, _, hgt) = col_str.rpartition('_')\n",
    "    new_vname = var_rename[vname]\n",
    "    new_hgt = int(float(hgt) * 100)\n",
    "\n",
    "    return f\"{new_vname}_{new_hgt}\"\n",
    "\n",
    "station = 'ES4'\n",
    "\n",
    "# File numbers are sequential\n",
    "file_list = sorted(glob.glob(os.path.join(in_path, station, 'raw_20hz', f\"DPG-UoU_{station}_*_20Hz_*.txt\")))\n",
    "\n",
    "file_dict = {k:{'start_end_dates':find_time_lims(k)} for k in file_list}\n",
    "\n",
    "date_list = []\n",
    "\n",
    "# Make sure output directories are there\n",
    "os.makedirs(os.path.join(parq_path, \"raw\", station), exist_ok=True)\n",
    "os.makedirs(os.path.join(csv_path, \"raw\", station), exist_ok=True)\n",
    "\n",
    "for i,f in enumerate(file_dic  t.keys()):\n",
    "\n",
    "    # Read in file\n",
    "    in_df = pd.read_csv(f, skiprows=[0,2,3], header=0, index_col=0, parse_dates=[0], na_values=[\"NAN\"])\n",
    "    \n",
    "    # Drop duplicates and shift one\n",
    "    in_df = in_df[~in_df.index.duplicated(keep='first')].shift(-1, freq='50ms')\n",
    "\n",
    "    # Write initial running dataframe or concat to existing\n",
    "    if i==0:\n",
    "        running_df = in_df.copy()\n",
    "    else:\n",
    "        running_df = pd.concat([in_df, running_df])\n",
    "\n",
    "    # Drop any dates already written out\n",
    "    # https://stackoverflow.com/questions/37307796/fastest-way-to-eliminate-specific-dates-from-pandas-dataframe\n",
    "    running_df = running_df.loc[~np.in1d(running_df.index.date, pd.to_datetime(date_list).date), :]\n",
    "\n",
    "    # Concat to running dataframe\n",
    "    out_dates = np.unique(running_df.index.date)\n",
    "\n",
    "    for j,date in enumerate(out_dates):\n",
    "        # Skip last date if not on last data file\n",
    "        if (j == out_dates.shape[0]-1) & (i != len(file_dict.keys())):\n",
    "            continue\n",
    "        \n",
    "        # Select date\n",
    "        temp_df = running_df.loc[date.strftime('%Y-%m-%d')]\n",
    "\n",
    "        # Shift back to correct datetimes, sort index\n",
    "        # Also force to 20hz between first and last value\n",
    "        temp_df = temp_df.shift(1, freq='50ms').sort_index().asfreq('50ms')\n",
    "\n",
    "        # Rename columns\n",
    "        temp_df = temp_df.rename(columns=rename_es4)\n",
    "    \n",
    "        # Write out to parquet file\n",
    "        file_date = date.strftime('%Y%m%d')\n",
    "\n",
    "        temp_df.to_parquet(os.path.join(parq_path, \"raw\", station, f\"{station}_EC_20hz_{file_date}_raw.parquet\"), engine='pyarrow', index=True)\n",
    "        temp_df.to_csv(os.path.join(csv_path, \"raw\", station, f\"{station}_EC_20hz_{file_date}_raw.csv\"), float_format='%g')\n",
    "\n",
    "        date_list.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES5 - UofU\n",
    "\n",
    "# Follow same procedure as ES1, but with kh2o and FW \n",
    "\n",
    "def rename_es5(col_str):\n",
    "    \n",
    "    var_hgt_rename = {'Ux' : 'u',\n",
    "                      'Uy' : 'v',\n",
    "                      'Uz' : 'w',\n",
    "                      'T_Sonic' : 't_sonic',\n",
    "                      'diagnostic' : 'diag_sonic',\n",
    "                      'FW' : 'fw'\n",
    "                      }\n",
    "\n",
    "    var_nohgt_rename = {'KH2O_H2O' : 'kh2o_h2o_200',\n",
    "                        'KH2O_mV' : 'kh2o_mv_200'\n",
    "                        }\n",
    "\n",
    "    (vname, _, hgt) = col_str.rpartition('_')\n",
    "\n",
    "    if vname in var_hgt_rename:\n",
    "\n",
    "        new_vname = var_hgt_rename[vname]\n",
    "        new_hgt = int(float(hgt) * 100)\n",
    "\n",
    "        return f\"{new_vname}_{new_hgt}\"\n",
    "    \n",
    "    elif vname=='KH2O':\n",
    "\n",
    "        return var_nohgt_rename[col_str]\n",
    "\n",
    "    else:\n",
    "\n",
    "        return col_str\n",
    "\n",
    "station = 'ES5'\n",
    "\n",
    "# First find files and start/end dates of each file\n",
    "file_list = sorted(glob.glob(os.path.join(in_path, station, 'raw_20hz', f\"UoU_{station}_*_20Hz_*.txt\")))\n",
    "\n",
    "file_dict = {k:{'start_end_dates':find_time_lims(k)} for k in file_list}\n",
    "\n",
    "date_list = []\n",
    "\n",
    "# Make sure output directories are there\n",
    "os.makedirs(os.path.join(parq_path, \"raw\", station), exist_ok=True)\n",
    "os.makedirs(os.path.join(csv_path, \"raw\", station), exist_ok=True)\n",
    "\n",
    "for i,f in enumerate(file_dict.keys()):\n",
    "\n",
    "    # Read in file\n",
    "    in_df = pd.read_csv(f, skiprows=[0,2,3], header=0, index_col=0, parse_dates=[0], na_values=[\"NAN\"])\n",
    "    \n",
    "    # Drop duplicates and shift one\n",
    "    in_df = in_df[~in_df.index.duplicated(keep='first')].shift(-1, freq='50ms')\n",
    "\n",
    "    # Write initial running dataframe or concat to existing\n",
    "    if i==0:\n",
    "        running_df = in_df.copy()\n",
    "    else:\n",
    "        running_df = pd.concat([in_df, running_df])\n",
    "\n",
    "    # Drop any dates already written out\n",
    "    # https://stackoverflow.com/questions/37307796/fastest-way-to-eliminate-specific-dates-from-pandas-dataframe\n",
    "    running_df = running_df.loc[~np.in1d(running_df.index.date, pd.to_datetime(date_list).date), :]\n",
    "\n",
    "    # Concat to running dataframe\n",
    "    out_dates = np.unique(running_df.index.date)\n",
    "\n",
    "    for j,date in enumerate(out_dates):\n",
    "        # Skip last date if not on last data file\n",
    "        if (j == out_dates.shape[0]-1) & (i != len(file_dict.keys())):\n",
    "            continue\n",
    "        \n",
    "        # Select date\n",
    "        temp_df = running_df.loc[date.strftime('%Y-%m-%d')]\n",
    "\n",
    "        # Shift back to correct datetimes, sort index\n",
    "        # Also force to 20hz between first and last value\n",
    "        temp_df = temp_df.shift(1, freq='50ms').sort_index().asfreq('50ms')\n",
    "\n",
    "        # Rename columns\n",
    "        temp_df = temp_df.rename(columns=rename_es5)\n",
    "    \n",
    "        # Write out to parquet file\n",
    "        file_date = date.strftime('%Y%m%d')\n",
    "\n",
    "        temp_df.to_parquet(os.path.join(parq_path, \"raw\", station, f\"{station}_EC_20hz_{file_date}_raw.parquet\"), engine='pyarrow', index=True)\n",
    "        temp_df.to_csv(os.path.join(csv_path, \"raw\", station, f\"{station}_EC_20hz_{file_date}_raw.csv\"), float_format='%g')\n",
    "\n",
    "        date_list.append(date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}