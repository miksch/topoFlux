{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd07be8e1c33fe0cf578c5fc47dbc2dc7281533f14aa3861876d8419516a6bb5098",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\moo90\\miniconda3\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from functools import partial\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, progress\n",
    "#client = Client(n_workers=2, threads_per_worker=2, memory_limit='4GB')\n",
    "#client"
   ]
  },
  {
   "source": [
    "## Script to clean up any data inconsistencies for UTESpac and any other use (e.g. python)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = \"C:/Users/moo90/Box/data/materhorn/raw_data/ES\"\n",
    "\n",
    "out_path = \"C:/Users/moo90/Box/data/materhorn/ES\"\n",
    "\n",
    "\n",
    "# UofU : var_hgt (w/ decimals)\n",
    "# Split on underscore, index by first and last\n",
    "# first -> use rename dict\n",
    "# last -> convert to float, then centimeters, then centimeters to int\n",
    "\n",
    "# UND : varhgt_?? (w/o decimals)\n",
    "# Split on underscore, use first, then split by numerical vs. alphabetic\n",
    "# alpha -> use rename dict\n",
    "# num -> convert to centimeters, then centimeters to int\n",
    "\n",
    "\n",
    "def find_time_lims(fpath, dt_str='%Y%m%d%H%M%S'):\n",
    "    \"\"\"\n",
    "    Find start and end dates for each file based on naming conventions for materhorn dataset\n",
    "\n",
    "    Args:\n",
    "        fpath (str) : full file name or path to dataset\n",
    "        dt_str (str) : strptime representation of the date in the filename\n",
    "\n",
    "    Returns:\n",
    "        list containing start [0] and end [1] of timeseries in file\n",
    "    \"\"\"\n",
    "\n",
    "    dt_list = os.path.splitext(os.path.basename(fpath))[0].split('_')[-2:]\n",
    "\n",
    "    return [pd.to_datetime(dt, format=dt_str) for dt in dt_list]\n",
    "\n",
    "def asdf(file_list, rename_func, in_path, parq_path, csv_path, station_pairs=None):\n",
    "\n",
    "    file_dict = {k:{'start_end_dates':find_time_lims(k)} for k in file_list}\n",
    "\n",
    "    date_list = []\n",
    "\n",
    "    # Make sure output directories are there\n",
    "    os.makedirs(os.path.join(parq_path, \"raw\", station), exist_ok=True)\n",
    "    os.makedirs(os.path.join(csv_path, \"raw\", station), exist_ok=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ES1 - UofU (easiest)\n",
    "\n",
    "def rename_es1(col_str):\n",
    "    \n",
    "    var_rename = {'Ux' : 'ux',\n",
    "                  'Uy' : 'uy',\n",
    "                  'Uz' : 'uz',\n",
    "                  'T_Sonic' : 't_sonic',\n",
    "                  'diagnostic' : 'diag_sonic'\n",
    "                  }\n",
    "\n",
    "    hgt_rename = {\n",
    "\n",
    "    }\n",
    "\n",
    "    (vname, _, hgt) = col_str.rpartition('_')\n",
    "    new_vname = var_rename[vname]\n",
    "    new_hgt = int(float(hgt) * 100)\n",
    "\n",
    "    return f\"{new_vname}_{new_hgt}\"\n",
    "\n",
    "station = 'ES1'\n",
    "\n",
    "# First find files and start/end dates of each file\n",
    "file_list = sorted(glob.glob(os.path.join(in_path, station, f\"DPG-UoU_{station}_*_20Hz_*.txt\")))\n",
    "\n",
    "file_dict = {k:{'start_end_dates':find_time_lims(k)} for k in file_list}\n",
    "\n",
    "date_list = []\n",
    "\n",
    "# Make sure output directories are there\n",
    "os.makedirs(os.path.join(out_path, station, \"raw_20hz\", \"parq\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(out_path, station, \"raw_20hz\", \"csv\"), exist_ok=True)\n",
    "\n",
    "for i,f in enumerate(file_dict.keys()):\n",
    "\n",
    "    # Read in file\n",
    "    in_df = pd.read_csv(f, skiprows=[0,2,3], header=0, index_col=0, parse_dates=[0], na_values=[\"NAN\"])\n",
    "    \n",
    "    # Drop duplicates, force to 20hz, and shift one\n",
    "    in_df = in_df[~in_df.index.duplicated(keep='first')].shift(-1, freq='50ms')\n",
    "\n",
    "    # Write initial running dataframe or concat to existing\n",
    "    if i==0:\n",
    "        running_df = in_df.copy()\n",
    "    else:\n",
    "        running_df = pd.concat([in_df, running_df])\n",
    "\n",
    "    # Drop any dates already written out\n",
    "    # https://stackoverflow.com/questions/37307796/fastest-way-to-eliminate-specific-dates-from-pandas-dataframe\n",
    "    running_df = running_df.loc[~np.in1d(running_df.index.date, pd.to_datetime(date_list).date), :]\n",
    "\n",
    "    # Concat to running dataframe\n",
    "    out_dates = np.unique(running_df.index.date)\n",
    "\n",
    "    for j,date in enumerate(out_dates):\n",
    "        # Skip last date if not on last data file\n",
    "        if (j == out_dates.shape[0]-1) & (i != len(file_dict.keys())):\n",
    "            continue\n",
    "        \n",
    "        # Select date\n",
    "        temp_df = running_df.loc[date.strftime('%Y-%m-%d')]\n",
    "\n",
    "        # Shift back to correct datetimes, sort index\n",
    "        # Also force to 20hz between first and last value\n",
    "        temp_df = temp_df.shift(1, freq='50ms').sort_index().asfreq('50ms')\n",
    "\n",
    "        # Rename columns\n",
    "        temp_df = temp_df.rename(columns=rename_es1)\n",
    "    \n",
    "        # Write out to parquet file\n",
    "        file_date = date.strftime('%Y_%m_%d')\n",
    "        temp_df.to_parquet(os.path.join(out_path, station, \"raw_20hz\", \"parq\", f\"{station}_EC_20hz_{file_date}.parquet\"), engine='pyarrow', index=True)\n",
    "        temp_df.to_csv(os.path.join(out_path, station, \"raw_20hz\", \"csv\", f\"{station}_EC_20hz_{file_date}.csv\"), float_format='%g')\n",
    "\n",
    "        date_list.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ES2 - UND\n",
    "\n",
    "def rename_es2(col_str, file_id):\n",
    "    \n",
    "    var_rename = {'u' : 'ux',\n",
    "                  'v' : 'uy',\n",
    "                  'w' : 'uz',\n",
    "                  'ts' : 't_sonic'\n",
    "                  }\n",
    "\n",
    "    try:\n",
    "        [vname, hgt] = re.findall(r\"[^\\W\\d_]+|\\d+\", col_str.rpartition('_')[0])\n",
    "        new_vname = var_rename[vname]\n",
    "\n",
    "        if hgt.startswith(\"0\"):\n",
    "            new_hgt = int(float(hgt) * 10)\n",
    "        else:\n",
    "            new_hgt = int(float(hgt) * 100)\n",
    "\n",
    "        return f\"{new_vname}_{new_hgt}\"\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        return f\"{col_str.lower()}_{file_id}\"\n",
    "\n",
    "# 1407 - Fall 0.5, 4, and 10 m\n",
    "# 2590 - Spring 16, 20, 25, and 32 m\n",
    "# 2717 - Fall 16, 20, 25, and 28 m and Spring 0.5, 2, 5, and 10 m\n",
    "# 2720 - Fall and Spring temperature profiles\n",
    "\n",
    "# Fall should zip fine\n",
    "# Spring is off by one (jfc)... decided to load all into memory\n",
    "station_pairs = {#2012 : [1407, 2717],\n",
    "                 2013 : [2717, 2590]\n",
    "                 }\n",
    "\n",
    "station = 'ES2'\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(os.path.join(out_path, station, \"raw_20hz\", \"parq\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(out_path, station, \"raw_20hz\", \"csv\"), exist_ok=True)\n",
    "\n",
    "for yr in station_pairs.keys():\n",
    "\n",
    "    yr_list = []\n",
    "\n",
    "    for fid in station_pairs[yr]:\n",
    "\n",
    "        temp_df_list = []\n",
    "\n",
    "        # Get file list for certain file id and year\n",
    "        file_list = glob.glob(os.path.join(in_path, station, 'raw_20hz', f\"UND_{station}_{fid}*_{yr}*.txt\"))\n",
    "\n",
    "        for f in file_list:\n",
    "            \n",
    "            # Read in data from file, remove any duplicates, shift -1 for date grouping, and rename columns\n",
    "            temp_df = pd.read_csv(f, skiprows=[0,2,3], header=0, na_values=[\"NAN\"], parse_dates=[0], index_col=[0])\n",
    "            temp_df = temp_df[~temp_df.index.duplicated(keep='first')].shift(-1, freq='50ms').rename(columns=partial(rename_es2, file_id=fid))\n",
    "\n",
    "            # Append to running list\n",
    "            temp_df_list.append(temp_df)\n",
    "\n",
    "        # Concat all dataframes with same year and \n",
    "        yr_list.append(pd.concat(temp_df_list))\n",
    "\n",
    "    # Concat all dataframes for the year, ort and pad any missing values with nans\n",
    "    yr_df = pd.concat(yr_list, axis=1)\n",
    "    yr_list = []\n",
    "    yr_df = yr_df.sort_index().asfreq('50ms')\n",
    "\n",
    "    # Get dates to loop through (shifting once just in case... although it's a very isolated edge case)\n",
    "    out_dates = np.unique(yr_df.shift(1, freq='50ms').index.date)\n",
    "\n",
    "    for date in out_dates:\n",
    "\n",
    "        # Subset by days, shift back times, and rename columns\n",
    "        daily_df = yr_df.loc[date.strftime('%Y-%m-%d')].shift(1, freq='50ms')\n",
    "\n",
    "        # Write out to parquet and csv files\n",
    "        file_date = date.strftime('%Y_%m_%d')\n",
    "        daily_df.to_parquet(os.path.join(out_path, station, \"raw_20hz\", \"parq\", f\"{station}_EC_20hz_{file_date}.parquet\"), engine='pyarrow', index=True)\n",
    "        daily_df.to_csv(os.path.join(out_path, station, \"raw_20hz\", \"csv\", f\"{station}_EC_20hz_{file_date}.csv\"), float_format='%g')\n",
    "\n",
    "    yr_df = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES3 - UND\n",
    "\n",
    "# 2590 - Fall temperature profiles\n",
    "# 5254 - Spring temperature profiles\n",
    "# 2667 - Fall and Spring 5, 10, and 20 m \n",
    "# 2712 - Fall and Spring 0.5 and 2 m (NOTE: 2m has different naming conventions (yay...))\n",
    "    # Fall \"TIMESTAMP\",\"RECORD\",\"u05_EM\",\"v05_EM\",\"w05_EM\",\"ts05_EM\",\"T2_EM\",\"Ux2_EM\",\"Uy2_EM\",\"Uz2_EM\",\"Ts2_EM\",\"rho_w\",\"kh_mV\",\"PTemp\",\"VWC\",\"Period\"\n",
    "    # Spring \"TIMESTAMP\",\"RECORD\",\"u05_ES3\",\"v05_ES3\",\"w05_ES3\",\"Ts05_ES3\",\"T2_ES3\",\"ux2_ES3\",\"Uy2_ES3\",\"Uz2_ES3\",\"Ts2_ES3\",\"CO2_ES3\",\"H2O_ES3\",\"CO2abs_ES3\",\"H2Oabs_ES3\",\"P7500_ES3\",\"T7500_ES3\",\"Cool7500_ES3\",\"Diag7500_ES3\",\"PTemp\"\n",
    "\n",
    "# NOTE: this only extracts the EC data, might do temperature data later on\n",
    "\n",
    "def rename_es3(col_str, file_id):\n",
    "    \n",
    "    var_rename_hgt = {'u' : 'ux',\n",
    "                      'v' : 'uy',\n",
    "                      'w' : 'uz',\n",
    "                      'ux' : 'ux',\n",
    "                      'uy' : 'uy',\n",
    "                      'uz' : 'uz',\n",
    "                      'ts' : 't_sonic'\n",
    "                     }\n",
    "\n",
    "    hgt_rename = {'05':'0.5',\n",
    "                  '2':'2',\n",
    "                  '5':'5',\n",
    "                  '10':'10',\n",
    "                  '20':'20'\n",
    "                 }\n",
    "\n",
    "    var_rename_nohgt = {'rho_w' : 'kh2o_h2o_2',\n",
    "                        'kh_mV' : 'kh2o_mV_2',\n",
    "                        'CO2_ES3' : 'li_co2_2',\n",
    "                        'H2O_ES3' : 'li_h2o_2',\n",
    "                        'CO2abs_ES3' : 'li_co2_abs_2',\n",
    "                        'H2Oabs_ES3' : 'li_h2o_abs_2',\n",
    "                        'P7500_ES3' : 'li_pres_2',\n",
    "                        'T7500_ES3' : 'li_t_2',\n",
    "                        'Cool7500_ES3' : 'li_cool_2',\n",
    "                        'Diag7500_ES3' : 'li_diag_2'\n",
    "                       }\n",
    "\n",
    "\n",
    "    # for sonic variables\n",
    "    try:\n",
    "        [vname, hgt] = re.findall(r\"[^\\W\\d_]+|\\d+\", col_str.rpartition('_')[0].lower())\n",
    "        \n",
    "        new_vname = var_rename_hgt[vname]\n",
    "        new_hgt = hgt_rename[hgt]\n",
    "\n",
    "        return f\"{new_vname}_{new_hgt}\"\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        # for kh2o and li7500 variables\n",
    "        try:\n",
    "            return var_rename_nohgt[col_str]\n",
    "        \n",
    "        # any others\n",
    "        except:\n",
    "            return f\"{col_str.lower()}_{file_id}\"\n",
    "\n",
    "\n",
    "# Force lowercase for Ts/ts comparison\n",
    "\n",
    "\n",
    "# UND_ES3_2590_20Hz_FluxTower_20120924184437_20120925162426.txt\n",
    "\n",
    "station_pairs = {2012: [2712, 2667],\n",
    "                 2013: [2712, 2667]\n",
    "                 }\n",
    "\n",
    "station = 'ES3'\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(os.path.join(out_path, station, \"raw_20hz\", \"parq\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(out_path, station, \"raw_20hz\", \"csv\"), exist_ok=True)\n",
    "\n",
    "# Follow same processing steps as ES2 (because again, the file lists are not the same length)\n",
    "for yr in station_pairs.keys():\n",
    "\n",
    "    yr_list = []\n",
    "\n",
    "    for fid in station_pairs[yr]:\n",
    "\n",
    "        temp_df_list = []\n",
    "\n",
    "        # Get file list for certain file id and year\n",
    "        file_list = glob.glob(os.path.join(in_path, station, 'raw_20hz', f\"UND_{station}_{fid}*_{yr}*.txt\"))\n",
    "\n",
    "        for f in file_list:\n",
    "            \n",
    "            # Read in data from file, remove any duplicates, shift -1 for date grouping, and rename columns\n",
    "            temp_df = pd.read_csv(f, skiprows=[0,2,3], header=0, na_values=[\"NAN\"], parse_dates=[0], index_col=[0])\n",
    "            temp_df = temp_df.loc[~temp_df.index.duplicated(keep='first')].shift(-1, freq='50ms').rename(columns=partial(rename_es3, file_id=fid))\n",
    "\n",
    "            # Append to running list\n",
    "            temp_df_list.append(temp_df)\n",
    "\n",
    "        # Concat all dataframes with same year, check any duplicate indexes again, and append to year list\n",
    "        temp_yr_concat = pd.concat(temp_df_list)\n",
    "        temp_yr_concat = temp_yr_concat.loc[~temp_yr_concat.index.duplicated(keep='first')]\n",
    "        yr_list.append(temp_yr_concat)\n",
    "\n",
    "    # Concat all dataframes for the year, ort and pad any missing values with nans\n",
    "    yr_df = pd.concat(yr_list, axis=1)\n",
    "    yr_list = []\n",
    "    yr_df = yr_df.sort_index().asfreq('50ms')\n",
    "\n",
    "    # Get dates to loop through (shifting once just in case... although it's a very isolated edge case)\n",
    "    out_dates = np.unique(yr_df.shift(1, freq='50ms').index.date)\n",
    "\n",
    "    for date in out_dates:\n",
    "\n",
    "        # Subset by days, shift back times, and rename columns\n",
    "        daily_df = yr_df.loc[date.strftime('%Y-%m-%d')].shift(1, freq='50ms')\n",
    "\n",
    "        # Write out to parquet and csv files\n",
    "        file_date = date.strftime('%Y_%m_%d')\n",
    "        daily_df.to_parquet(os.path.join(out_path, station, \"raw_20hz\", \"parq\", f\"{station}_EC_20hz_{file_date}.parquet\"), engine='pyarrow', index=True)\n",
    "        daily_df.to_csv(os.path.join(out_path, station, \"raw_20hz\", \"csv\", f\"{station}_EC_20hz_{file_date}.csv\"), float_format='%g')\n",
    "\n",
    "    yr_df = None\n",
    "\n",
    "# Clear out variables\n",
    "yr_df = None\n",
    "yr_list = None\n",
    "temp_df_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[65338088, 60738567]\n"
     ]
    }
   ],
   "source": [
    "print([l.shape[0] for l in yr_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ES4 - UofU\n",
    "\n",
    "# NOTE: tower fell down at one point\n",
    "# 3063 - Fall 0.5, 2, 5, 10, 20, and 32 m\n",
    "# 4004 - Spring 0.5, 2, 5, 10, 20, and 32 m (pt 1)\n",
    "# 5252 - Spring 0.5, 2, 5, 10, 20, and 32 m (pt 2)\n",
    "\n",
    "def rename_es4(col_str):\n",
    "    \n",
    "    var_rename = {'Ux' : 'ux',\n",
    "                  'Uy' : 'uy',\n",
    "                  'Uz' : 'uz',\n",
    "                  'T_Sonic' : 't_sonic',\n",
    "                  'diagnostic' : 'diag_sonic'\n",
    "                  }\n",
    "\n",
    "\n",
    "    hgt_rename = {'05':'0.5',\n",
    "                  '2':'2',\n",
    "                  '5':'5',\n",
    "                  '10':'10',\n",
    "                  '20':'20',\n",
    "                  '32':'32'\n",
    "                 }\n",
    "\n",
    "    (vname, _, hgt) = col_str.rpartition('_')\n",
    "    new_vname = var_rename[vname]\n",
    "    new_hgt = int(float(hgt) * 100)\n",
    "\n",
    "    return f\"{new_vname}_{new_hgt}\"\n",
    "\n",
    "station = 'ES4'\n",
    "\n",
    "# File numbers are sequential\n",
    "file_list = sorted(glob.glob(os.path.join(in_path, station, 'raw_20hz', f\"DPG-UoU_{station}_*_20Hz_*.txt\")))\n",
    "\n",
    "file_dict = {k:{'start_end_dates':find_time_lims(k)} for k in file_list}\n",
    "\n",
    "date_list = []\n",
    "\n",
    "# Make sure output directories are there\n",
    "os.makedirs(os.path.join(out_path, station, \"raw_20hz\", \"parq\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(out_path, station, \"raw_20hz\", \"csv\"), exist_ok=True)\n",
    "\n",
    "for i,f in enumerate(file_dic  t.keys()):\n",
    "\n",
    "    # Read in file\n",
    "    in_df = pd.read_csv(f, skiprows=[0,2,3], header=0, index_col=0, parse_dates=[0], na_values=[\"NAN\"])\n",
    "    \n",
    "    # Drop duplicates and shift one\n",
    "    in_df = in_df[~in_df.index.duplicated(keep='first')].shift(-1, freq='50ms')\n",
    "\n",
    "    # Write initial running dataframe or concat to existing\n",
    "    if i==0:\n",
    "        running_df = in_df.copy()\n",
    "    else:\n",
    "        running_df = pd.concat([in_df, running_df])\n",
    "\n",
    "    # Drop any dates already written out\n",
    "    # https://stackoverflow.com/questions/37307796/fastest-way-to-eliminate-specific-dates-from-pandas-dataframe\n",
    "    running_df = running_df.loc[~np.in1d(running_df.index.date, pd.to_datetime(date_list).date), :]\n",
    "\n",
    "    # Concat to running dataframe\n",
    "    out_dates = np.unique(running_df.index.date)\n",
    "\n",
    "    for j,date in enumerate(out_dates):\n",
    "        # Skip last date if not on last data file\n",
    "        if (j == out_dates.shape[0]-1) & (i != len(file_dict.keys())):\n",
    "            continue\n",
    "        \n",
    "        # Select date\n",
    "        temp_df = running_df.loc[date.strftime('%Y-%m-%d')]\n",
    "\n",
    "        # Shift back to correct datetimes, sort index\n",
    "        # Also force to 20hz between first and last value\n",
    "        temp_df = temp_df.shift(1, freq='50ms').sort_index().asfreq('50ms')\n",
    "\n",
    "        # Rename columns\n",
    "        temp_df = temp_df.rename(columns=rename_es4)\n",
    "    \n",
    "        # Write out to parquet file\n",
    "        file_date = date.strftime('%Y_%m_%d')\n",
    "\n",
    "        temp_df.to_parquet(os.path.join(out_path, station, \"raw_20hz\", \"parq\", f\"{station}_EC_20hz_{file_date}.parquet\"), engine='pyarrow', index=True)\n",
    "        temp_df.to_csv(os.path.join(out_path, station, \"raw_20hz\", \"csv\", f\"{station}_EC_20hz_{file_date}.csv\"), float_format='%g')\n",
    "\n",
    "        date_list.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES5 - UofU\n",
    "\n",
    "# Follow same procedure as ES1, but with kh2o and FW \n",
    "\n",
    "def rename_es5(col_str):\n",
    "    \n",
    "    var_hgt_rename = {'Ux' : 'ux',\n",
    "                      'Uy' : 'uy',\n",
    "                      'Uz' : 'uz',\n",
    "                      'T_Sonic' : 't_sonic',\n",
    "                      'diagnostic' : 'diag_sonic',\n",
    "                      'FW' : 'fw'\n",
    "                      }\n",
    "\n",
    "    var_nohgt_rename = {'KH2O_H2O' : 'kh2o_h2o_2',\n",
    "                        'KH2O_mV' : 'kh2o_mv_2'\n",
    "                        }\n",
    "\n",
    "    (vname, _, hgt) = col_str.rpartition('_')\n",
    "\n",
    "    if vname in var_hgt_rename:\n",
    "\n",
    "        new_vname = var_hgt_rename[vname]\n",
    "        new_hgt = int(float(hgt) * 100)\n",
    "\n",
    "        return f\"{new_vname}_{new_hgt}\"\n",
    "    \n",
    "    elif vname=='KH2O':\n",
    "\n",
    "        return var_nohgt_rename[col_str]\n",
    "\n",
    "    else:\n",
    "\n",
    "        return col_str\n",
    "\n",
    "station = 'ES5'\n",
    "\n",
    "# First find files and start/end dates of each file\n",
    "file_list = sorted(glob.glob(os.path.join(in_path, station, 'raw_20hz', f\"UoU_{station}_*_20Hz_*.txt\")))\n",
    "\n",
    "file_dict = {k:{'start_end_dates':find_time_lims(k)} for k in file_list}\n",
    "\n",
    "date_list = []\n",
    "\n",
    "# Make sure output directories are there\n",
    "os.makedirs(os.path.join(out_path, station, \"raw_20hz\", \"parq\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(out_path, station, \"raw_20hz\", \"csv\"), exist_ok=True)\n",
    "\n",
    "for i,f in enumerate(file_dict.keys()):\n",
    "\n",
    "    # Read in file\n",
    "    in_df = pd.read_csv(f, skiprows=[0,2,3], header=0, index_col=0, parse_dates=[0], na_values=[\"NAN\"])\n",
    "    \n",
    "    # Drop duplicates and shift one\n",
    "    in_df = in_df[~in_df.index.duplicated(keep='first')].shift(-1, freq='50ms')\n",
    "\n",
    "    # Write initial running dataframe or concat to existing\n",
    "    if i==0:\n",
    "        running_df = in_df.copy()\n",
    "    else:\n",
    "        running_df = pd.concat([in_df, running_df])\n",
    "\n",
    "    # Drop any dates already written out\n",
    "    # https://stackoverflow.com/questions/37307796/fastest-way-to-eliminate-specific-dates-from-pandas-dataframe\n",
    "    running_df = running_df.loc[~np.in1d(running_df.index.date, pd.to_datetime(date_list).date), :]\n",
    "\n",
    "    # Concat to running dataframe\n",
    "    out_dates = np.unique(running_df.index.date)\n",
    "\n",
    "    for j,date in enumerate(out_dates):\n",
    "        # Skip last date if not on last data file\n",
    "        if (j == out_dates.shape[0]-1) & (i != len(file_dict.keys())):\n",
    "            continue\n",
    "        \n",
    "        # Select date\n",
    "        temp_df = running_df.loc[date.strftime('%Y-%m-%d')]\n",
    "\n",
    "        # Shift back to correct datetimes, sort index\n",
    "        # Also force to 20hz between first and last value\n",
    "        temp_df = temp_df.shift(1, freq='50ms').sort_index().asfreq('50ms')\n",
    "\n",
    "        # Rename columns\n",
    "        temp_df = temp_df.rename(columns=rename_es5)\n",
    "    \n",
    "        # Write out to parquet file\n",
    "        file_date = date.strftime('%Y_%m_%d')\n",
    "\n",
    "        temp_df.to_parquet(os.path.join(out_path, station, \"raw_20hz\", \"parq\", f\"{station}_EC_20hz_{file_date}.parquet\"), engine='pyarrow', index=True)\n",
    "        temp_df.to_csv(os.path.join(out_path, station, \"raw_20hz\", \"csv\", f\"{station}_EC_20hz_{file_date}.csv\"), float_format='%g')\n",
    "\n",
    "        date_list.append(date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}